"""
ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒ»ç•°å¸¸å€¤ã‚’æ¤œè¨¼
"""

import pandas as pd
import numpy as np
import argparse
from pathlib import Path
import sys
import json

def validate_price_data(file_path):
    """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVï¼‰ã‚’æ¤œè¨¼
    
    æ¤œè¨¼é …ç›®:
    - å¿…é ˆã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèª
    - ä¾¡æ ¼æ•´åˆæ€§ï¼ˆhigh >= open, high >= low, high >= close, low <= open, low <= closeï¼‰
    - éè² åˆ¶ç´„ï¼ˆã™ã¹ã¦ã®ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ >= 0ï¼‰
    - å‰æ—¥æ¯”åˆ¶ç´„ï¼ˆ|å¤‰å‹•ç‡| <= 50%ï¼‰
    - éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ç¢ºèªï¼ˆ9501.T ã¾ãŸã¯ 9502.Tï¼‰
    
    Returns:
        (is_valid, errors): æ¤œè¨¼æˆåŠŸãƒ•ãƒ©ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
    """
    errors = []
    
    try:
        df = pd.read_csv(file_path, comment='#')
    except Exception as e:
        return False, [f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"]
    
    # å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèª
    required = ['date', 'open', 'high', 'low', 'close', 'volume']
    missing_cols = [col for col in required if col.lower() not in [c.lower() for c in df.columns]]
    if missing_cols:
        errors.append(f"å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³: {missing_cols}")
        return False, errors
    
    # ã‚«ãƒ©ãƒ åã‚’å°æ–‡å­—ã«çµ±ä¸€
    df.columns = df.columns.str.lower()
    
    # ä¾¡æ ¼æ•´åˆæ€§
    if not (df['high'] >= df['open']).all():
        errors.append("high < open ã®è¡ŒãŒå­˜åœ¨ã—ã¾ã™")
    if not (df['high'] >= df['low']).all():
        errors.append("high < low ã®è¡ŒãŒå­˜åœ¨ã—ã¾ã™")
    if not (df['high'] >= df['close']).all():
        errors.append("high < close ã®è¡ŒãŒå­˜åœ¨ã—ã¾ã™")
    if not (df['low'] <= df['open']).all():
        errors.append("low > open ã®è¡ŒãŒå­˜åœ¨ã—ã¾ã™")
    if not (df['low'] <= df['close']).all():
        errors.append("low > close ã®è¡ŒãŒå­˜åœ¨ã—ã¾ã™")
    
    # éè² åˆ¶ç´„
    for col in ['open', 'high', 'low', 'close', 'volume']:
        if (df[col] < 0).any():
            errors.append(f"{col} ã«è² ã®å€¤ãŒå­˜åœ¨ã—ã¾ã™")
    
    # å‰æ—¥æ¯”åˆ¶ç´„ï¼ˆç•°å¸¸å€¤æ¤œå‡ºï¼‰
    df = df.sort_values('date').reset_index(drop=True)
    df['pct_change'] = df['close'].pct_change()
    
    anomalies = df[df['pct_change'].abs() > 0.5]
    if len(anomalies) > 0:
        for idx, row in anomalies.iterrows():
            errors.append(f"ç•°å¸¸ãªå¤‰å‹•ç‡: {row['date']} å¤‰å‹•ç‡={row['pct_change']*100:.1f}%")
    
    # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ç¢ºèª
    if 'symbol' in df.columns:
        invalid_symbols = df[~df['symbol'].isin(['9501.T', '9502.T'])]
        if len(invalid_symbols) > 0:
            errors.append(f"ç„¡åŠ¹ãªéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰: {invalid_symbols['symbol'].unique().tolist()}")
    
    return len(errors) == 0, errors


def validate_indicator_data(file_path, indicator_type):
    """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVï¼‰ã‚’æ¤œè¨¼
    
    Args:
        file_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        indicator_type: 'sma', 'rsi', 'macd', 'bb' ã®ã„ãšã‚Œã‹
    
    Returns:
        (is_valid, errors): æ¤œè¨¼æˆåŠŸãƒ•ãƒ©ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
    """
    errors = []
    
    try:
        df = pd.read_csv(file_path, comment='#')
    except Exception as e:
        return False, [f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"]
    
    # ã‚«ãƒ©ãƒ åã‚’å°æ–‡å­—ã«çµ±ä¸€
    df.columns = df.columns.str.lower()
    
    # æŒ‡æ¨™ã‚¿ã‚¤ãƒ—åˆ¥ã®æ¤œè¨¼
    if indicator_type == 'sma':
        required = ['date', 'sma_5', 'sma_25', 'sma_75']
        for col in required:
            if col not in df.columns:
                errors.append(f"å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³: {col}")
        # SMAã¯éè² 
        for col in ['sma_5', 'sma_25', 'sma_75']:
            if col in df.columns and (df[col].dropna() < 0).any():
                errors.append(f"{col} ã«è² ã®å€¤ãŒå­˜åœ¨ã—ã¾ã™")
    
    elif indicator_type == 'rsi':
        required = ['date', 'rsi']
        for col in required:
            if col not in df.columns:
                errors.append(f"å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³: {col}")
        # RSIã¯0ï½100
        if 'rsi' in df.columns:
            rsi_out_of_range = df[(df['rsi'] < 0) | (df['rsi'] > 100)].dropna(subset=['rsi'])
            if len(rsi_out_of_range) > 0:
                errors.append(f"RSIãŒç¯„å›²å¤–ï¼ˆ0ï½100ï¼‰: {len(rsi_out_of_range)}è¡Œ")
    
    elif indicator_type == 'macd':
        required = ['date', 'macd', 'macd_signal', 'macd_hist']
        for col in required:
            if col not in df.columns:
                errors.append(f"å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³: {col}")
        # MACDãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  = MACD - ã‚·ã‚°ãƒŠãƒ«
        if all(col in df.columns for col in ['macd', 'macd_signal', 'macd_hist']):
            df_clean = df.dropna(subset=['macd', 'macd_signal', 'macd_hist'])
            calculated_hist = df_clean['macd'] - df_clean['macd_signal']
            diff = (df_clean['macd_hist'] - calculated_hist).abs()
            if (diff > 0.01).any():
                errors.append("MACDãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®è¨ˆç®—ãŒä¸æ­£ç¢ºã§ã™")
    
    elif indicator_type == 'bb':
        required = ['date', 'bb_upper', 'bb_middle', 'bb_lower']
        for col in required:
            if col not in df.columns:
                errors.append(f"å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³: {col}")
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰: bb_upper >= bb_middle >= bb_lower
        if all(col in df.columns for col in required):
            df_clean = df.dropna(subset=['bb_upper', 'bb_middle', 'bb_lower'])
            if not (df_clean['bb_upper'] >= df_clean['bb_middle']).all():
                errors.append("bb_upper < bb_middle ã®è¡ŒãŒå­˜åœ¨ã—ã¾ã™")
            if not (df_clean['bb_middle'] >= df_clean['bb_lower']).all():
                errors.append("bb_middle < bb_lower ã®è¡ŒãŒå­˜åœ¨ã—ã¾ã™")
    
    return len(errors) == 0, errors


def main():
    parser = argparse.ArgumentParser(description='ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--symbols', type=str, default='9501.T,9502.T',
                        help='éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    parser.add_argument('--price-dir', type=str, default='data/price',
                        help='æ ªä¾¡CSVãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--indicator-dir', type=str, default='data/indicators',
                        help='æŒ‡æ¨™CSVãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--output', type=str, default='data/validation_report.json',
                        help='æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‘ã‚¹ï¼ˆJSONï¼‰')
    
    args = parser.parse_args()
    symbols = [s.strip() for s in args.symbols.split(',')]
    
    print(f"ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼é–‹å§‹")
    print(f"å¯¾è±¡éŠ˜æŸ„: {symbols}\n")
    
    report = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'results': []
    }
    
    total_errors = 0
    
    for symbol in symbols:
        print(f"--- {symbol} ---")
        
        # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        price_file = Path(args.price_dir) / f"{symbol}.csv"
        if price_file.exists():
            is_valid, errors = validate_price_data(price_file)
            status = "âœ… æ­£å¸¸" if is_valid else "âŒ ã‚¨ãƒ©ãƒ¼"
            print(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {status}")
            if errors:
                for err in errors:
                    print(f"  - {err}")
                total_errors += len(errors)
            
            report['results'].append({
                'file': str(price_file),
                'type': 'price',
                'valid': is_valid,
                'errors': errors
            })
        else:
            print(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨ {price_file}")
            report['results'].append({
                'file': str(price_file),
                'type': 'price',
                'valid': False,
                'errors': ['ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“']
            })
            total_errors += 1
        
        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™æ¤œè¨¼
        indicators = {
            'sma': f"{symbol}_sma.csv",
            'rsi': f"{symbol}_rsi.csv",
            'macd': f"{symbol}_macd.csv",
            'bb': f"{symbol}_bb.csv"
        }
        
        for indicator_type, filename in indicators.items():
            indicator_file = Path(args.indicator_dir) / filename
            if indicator_file.exists():
                is_valid, errors = validate_indicator_data(indicator_file, indicator_type)
                status = "âœ… æ­£å¸¸" if is_valid else "âŒ ã‚¨ãƒ©ãƒ¼"
                print(f"{indicator_type.upper()}ãƒ‡ãƒ¼ã‚¿: {status}")
                if errors:
                    for err in errors:
                        print(f"  - {err}")
                    total_errors += len(errors)
                
                report['results'].append({
                    'file': str(indicator_file),
                    'type': indicator_type,
                    'valid': is_valid,
                    'errors': errors
                })
            else:
                print(f"{indicator_type.upper()}ãƒ‡ãƒ¼ã‚¿: âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨")
        
        print()
    
    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ: {output_path}")
    print(f"ç·ã‚¨ãƒ©ãƒ¼æ•°: {total_errors}")
    
    if total_errors > 0:
        print("âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å¤±æ•—")
        sys.exit(1)
    else:
        print("âœ… ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã§ã™")


if __name__ == "__main__":
    main()
